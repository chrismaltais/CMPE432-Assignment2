[root@sandbox-hdp ~]# spark-submit ./SparkRDD_old.py
SPARK_MAJOR_VERSION is set to 2, using Spark2
18/11/05 18:07:59 INFO SparkContext: Running Spark version 2.2.0.2.6.4.0-91
18/11/05 18:08:01 INFO SparkContext: Submitted application: Cisc432Spark
18/11/05 18:08:01 INFO SecurityManager: Changing view acls to: root
18/11/05 18:08:01 INFO SecurityManager: Changing modify acls to: root
18/11/05 18:08:01 INFO SecurityManager: Changing view acls groups to: 
18/11/05 18:08:01 INFO SecurityManager: Changing modify acls groups to: 
18/11/05 18:08:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
18/11/05 18:08:02 INFO Utils: Successfully started service 'sparkDriver' on port 46838.
18/11/05 18:08:02 INFO SparkEnv: Registering MapOutputTracker
18/11/05 18:08:02 INFO SparkEnv: Registering BlockManagerMaster
18/11/05 18:08:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/11/05 18:08:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/11/05 18:08:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1009a78c-7d5e-4d8f-890c-4e2194f5a7c2
18/11/05 18:08:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
18/11/05 18:08:03 INFO SparkEnv: Registering OutputCommitCoordinator
18/11/05 18:08:03 INFO log: Logging initialized @7777ms
18/11/05 18:08:03 INFO Server: jetty-9.3.z-SNAPSHOT
18/11/05 18:08:03 INFO Server: Started @8011ms
18/11/05 18:08:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
18/11/05 18:08:03 INFO AbstractConnector: Started ServerConnector@6acbf259{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
18/11/05 18:08:03 INFO Utils: Successfully started service 'SparkUI' on port 4041.
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3d3a1c56{/jobs,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2619a996{/jobs/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@21c8f9bf{/jobs/job,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@24adbe4{/jobs/job/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4b5157bd{/stages,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@29e04fc4{/stages/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@42b68fcd{/stages/stage,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@363882d5{/stages/stage/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@32b225d4{/stages/pool,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3532dbb2{/stages/pool/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@ee321db{/storage,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1c4d8769{/storage/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@47ce0ad0{/storage/rdd,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@15c3360{/storage/rdd/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@46901876{/environment,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@36f691f{/environment/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4c6cc0a4{/executors,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@13d58ada{/executors/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7afd21d4{/executors/threadDump,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7ff8e5af{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4997b3c6{/static,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@177e351c{/,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@369c1359{/api,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1ee77997{/jobs/job/kill,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ea5a26e{/stages/stage/kill,null,AVAILABLE,@Spark}
18/11/05 18:08:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.17.0.2:4041
18/11/05 18:08:04 INFO SparkContext: Added file file:/root/./SparkRDD_old.py at file:/root/./SparkRDD_old.py with timestamp 1541441284888
18/11/05 18:08:04 INFO Utils: Copying /root/SparkRDD_old.py to /tmp/spark-ecbcbd72-949f-44ea-b122-0ea52cf94259/userFiles-b846f094-fd56-49f0-9f7b-d8cfc8559174/SparkRDD_old.py
18/11/05 18:08:05 INFO Executor: Starting executor ID driver on host localhost
18/11/05 18:08:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42157.
18/11/05 18:08:05 INFO NettyBlockTransferService: Server created on 172.17.0.2:42157
18/11/05 18:08:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/11/05 18:08:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.17.0.2, 42157, None)
18/11/05 18:08:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:42157 with 366.3 MB RAM, BlockManagerId(driver, 172.17.0.2, 42157, None)
18/11/05 18:08:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.17.0.2, 42157, None)
18/11/05 18:08:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.17.0.2, 42157, None)
18/11/05 18:08:06 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@55d07ef3{/metrics/json,null,AVAILABLE,@Spark}
18/11/05 18:08:08 INFO EventLoggingListener: Logging events to hdfs:///spark2-history/local-1541441285162
/usr/hdp/current/spark2-client/python/lib/pyspark.zip/pyspark/context.py:205: UserWarning: Support for Python 2.6 is deprecated as of Spark 2.0.0
  warnings.warn("Support for Python 2.6 is deprecated as of Spark 2.0.0")
18/11/05 18:08:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 356.7 KB, free 366.0 MB)
18/11/05 18:08:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.1 KB, free 365.9 MB)
18/11/05 18:08:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.17.0.2:42157 (size: 32.1 KB, free: 366.3 MB)
18/11/05 18:08:09 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0
18/11/05 18:08:10 INFO FileInputFormat: Total input paths to process : 1
18/11/05 18:08:10 INFO SparkContext: Starting job: sortBy at /root/./SparkRDD_old.py:57
18/11/05 18:08:10 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /root/./SparkRDD_old.py:53)
18/11/05 18:08:10 INFO DAGScheduler: Got job 0 (sortBy at /root/./SparkRDD_old.py:57) with 2 output partitions
18/11/05 18:08:10 INFO DAGScheduler: Final stage: ResultStage 1 (sortBy at /root/./SparkRDD_old.py:57)
18/11/05 18:08:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
18/11/05 18:08:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
18/11/05 18:08:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /root/./SparkRDD_old.py:53), which has no missing parents
18/11/05 18:08:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.1 KB, free 365.9 MB)
18/11/05 18:08:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.7 KB, free 365.9 MB)
18/11/05 18:08:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.17.0.2:42157 (size: 5.7 KB, free: 366.3 MB)
18/11/05 18:08:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
18/11/05 18:08:10 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /root/./SparkRDD_old.py:53) (first 15 tasks are for partitions Vector(0, 1))
18/11/05 18:08:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
18/11/05 18:08:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4875 bytes)
18/11/05 18:08:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4875 bytes)
18/11/05 18:08:10 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
18/11/05 18:08:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
18/11/05 18:08:10 INFO Executor: Fetching file:/root/./SparkRDD_old.py with timestamp 1541441284888
18/11/05 18:08:11 INFO Utils: /root/./SparkRDD_old.py has been previously copied to /tmp/spark-ecbcbd72-949f-44ea-b122-0ea52cf94259/userFiles-b846f094-fd56-49f0-9f7b-d8cfc8559174/SparkRDD_old.py
18/11/05 18:08:11 INFO HadoopRDD: Input split: hdfs://sandbox-hdp.hortonworks.com:8020/user/assignment2/netIDs.dat:623829+623830
18/11/05 18:08:11 INFO HadoopRDD: Input split: hdfs://sandbox-hdp.hortonworks.com:8020/user/assignment2/netIDs.dat:0+623829
18/11/05 18:08:13 INFO PythonRunner: Times: total = 1466, boot = 550, init = 110, finish = 806
18/11/05 18:08:13 INFO PythonRunner: Times: total = 1585, boot = 755, init = 129, finish = 701
18/11/05 18:08:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1609 bytes result sent to driver
18/11/05 18:08:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1609 bytes result sent to driver
18/11/05 18:08:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2795 ms on localhost (executor driver) (1/2)
18/11/05 18:08:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2855 ms on localhost (executor driver) (2/2)
18/11/05 18:08:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
18/11/05 18:08:13 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /root/./SparkRDD_old.py:53) finished in 2.896 s
18/11/05 18:08:13 INFO DAGScheduler: looking for newly runnable stages
18/11/05 18:08:13 INFO DAGScheduler: running: Set()
18/11/05 18:08:13 INFO DAGScheduler: waiting: Set(ResultStage 1)
18/11/05 18:08:13 INFO DAGScheduler: failed: Set()
18/11/05 18:08:13 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortBy at /root/./SparkRDD_old.py:57), which has no missing parents
18/11/05 18:08:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 8.0 KB, free 365.9 MB)
18/11/05 18:08:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.9 KB, free 365.9 MB)
18/11/05 18:08:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.17.0.2:42157 (size: 4.9 KB, free: 366.3 MB)
18/11/05 18:08:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
18/11/05 18:08:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortBy at /root/./SparkRDD_old.py:57) (first 15 tasks are for partitions Vector(0, 1))
18/11/05 18:08:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
18/11/05 18:08:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4621 bytes)
18/11/05 18:08:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4621 bytes)
18/11/05 18:08:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
18/11/05 18:08:13 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
18/11/05 18:08:13 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:13 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 41 ms
18/11/05 18:08:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 28 ms
18/11/05 18:08:13 INFO PythonRunner: Times: total = 43, boot = -777, init = 810, finish = 10
18/11/05 18:08:13 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1569 bytes result sent to driver
18/11/05 18:08:13 INFO PythonRunner: Times: total = 70, boot = -796, init = 855, finish = 11
18/11/05 18:08:13 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1526 bytes result sent to driver
18/11/05 18:08:14 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 239 ms on localhost (executor driver) (1/2)
18/11/05 18:08:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 247 ms on localhost (executor driver) (2/2)
18/11/05 18:08:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
18/11/05 18:08:14 INFO DAGScheduler: ResultStage 1 (sortBy at /root/./SparkRDD_old.py:57) finished in 0.277 s
18/11/05 18:08:14 INFO DAGScheduler: Job 0 finished: sortBy at /root/./SparkRDD_old.py:57, took 3.634294 s
18/11/05 18:08:14 INFO SparkContext: Starting job: sortBy at /root/./SparkRDD_old.py:57
18/11/05 18:08:14 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 157 bytes
18/11/05 18:08:14 INFO DAGScheduler: Got job 1 (sortBy at /root/./SparkRDD_old.py:57) with 2 output partitions
18/11/05 18:08:14 INFO DAGScheduler: Final stage: ResultStage 3 (sortBy at /root/./SparkRDD_old.py:57)
18/11/05 18:08:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
18/11/05 18:08:14 INFO DAGScheduler: Missing parents: List()
18/11/05 18:08:14 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortBy at /root/./SparkRDD_old.py:57), which has no missing parents
18/11/05 18:08:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.9 KB, free 365.9 MB)
18/11/05 18:08:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.9 KB, free 365.9 MB)
18/11/05 18:08:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.17.0.2:42157 (size: 4.9 KB, free: 366.3 MB)
18/11/05 18:08:14 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
18/11/05 18:08:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortBy at /root/./SparkRDD_old.py:57) (first 15 tasks are for partitions Vector(0, 1))
18/11/05 18:08:14 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
18/11/05 18:08:14 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 4621 bytes)
18/11/05 18:08:14 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 4621 bytes)
18/11/05 18:08:14 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)
18/11/05 18:08:14 INFO Executor: Running task 1.0 in stage 3.0 (TID 5)
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/11/05 18:08:14 INFO PythonRunner: Times: total = 56, boot = -266, init = 316, finish = 6
18/11/05 18:08:14 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 2081 bytes result sent to driver
18/11/05 18:08:14 INFO PythonRunner: Times: total = 49, boot = -244, init = 287, finish = 6
18/11/05 18:08:14 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 110 ms on localhost (executor driver) (1/2)
18/11/05 18:08:14 INFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 2039 bytes result sent to driver
18/11/05 18:08:14 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 117 ms on localhost (executor driver) (2/2)
18/11/05 18:08:14 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
18/11/05 18:08:14 INFO DAGScheduler: ResultStage 3 (sortBy at /root/./SparkRDD_old.py:57) finished in 0.121 s
18/11/05 18:08:14 INFO DAGScheduler: Job 1 finished: sortBy at /root/./SparkRDD_old.py:57, took 0.176154 s
18/11/05 18:08:14 INFO SparkContext: Starting job: runJob at PythonRDD.scala:455
18/11/05 18:08:14 INFO DAGScheduler: Registering RDD 9 (sortBy at /root/./SparkRDD_old.py:57)
18/11/05 18:08:14 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:455) with 1 output partitions
18/11/05 18:08:14 INFO DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:455)
18/11/05 18:08:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
18/11/05 18:08:14 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
18/11/05 18:08:14 INFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /root/./SparkRDD_old.py:57), which has no missing parents
18/11/05 18:08:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 8.8 KB, free 365.9 MB)
18/11/05 18:08:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KB, free 365.9 MB)
18/11/05 18:08:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.17.0.2:42157 (size: 5.5 KB, free: 366.2 MB)
18/11/05 18:08:14 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
18/11/05 18:08:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /root/./SparkRDD_old.py:57) (first 15 tasks are for partitions Vector(0, 1))
18/11/05 18:08:14 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
18/11/05 18:08:14 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, ANY, 4610 bytes)
18/11/05 18:08:14 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, executor driver, partition 1, ANY, 4610 bytes)
18/11/05 18:08:14 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
18/11/05 18:08:14 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/11/05 18:08:14 INFO PythonRunner: Times: total = 61, boot = -157, init = 214, finish = 4
18/11/05 18:08:14 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 1695 bytes result sent to driver
18/11/05 18:08:14 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 159 ms on localhost (executor driver) (1/2)
18/11/05 18:08:14 INFO PythonRunner: Times: total = 73, boot = -189, init = 250, finish = 12
18/11/05 18:08:14 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 1695 bytes result sent to driver
18/11/05 18:08:14 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 228 ms on localhost (executor driver) (2/2)
18/11/05 18:08:14 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/11/05 18:08:14 INFO DAGScheduler: ShuffleMapStage 5 (sortBy at /root/./SparkRDD_old.py:57) finished in 0.229 s
18/11/05 18:08:14 INFO DAGScheduler: looking for newly runnable stages
18/11/05 18:08:14 INFO DAGScheduler: running: Set()
18/11/05 18:08:14 INFO DAGScheduler: waiting: Set(ResultStage 6)
18/11/05 18:08:14 INFO DAGScheduler: failed: Set()
18/11/05 18:08:14 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:48), which has no missing parents
18/11/05 18:08:14 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 6.8 KB, free 365.9 MB)
18/11/05 18:08:14 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 365.9 MB)
18/11/05 18:08:14 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.17.0.2:42157 (size: 4.2 KB, free: 366.2 MB)
18/11/05 18:08:14 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
18/11/05 18:08:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:48) (first 15 tasks are for partitions Vector(0))
18/11/05 18:08:14 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
18/11/05 18:08:14 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, executor driver, partition 0, ANY, 4621 bytes)
18/11/05 18:08:14 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
18/11/05 18:08:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
18/11/05 18:08:14 INFO PythonRunner: Times: total = 50, boot = -30, init = 78, finish = 2
18/11/05 18:08:14 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 2128 bytes result sent to driver
18/11/05 18:08:14 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 83 ms on localhost (executor driver) (1/1)
18/11/05 18:08:14 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
18/11/05 18:08:14 INFO DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:455) finished in 0.084 s
18/11/05 18:08:14 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:455, took 0.420931 s
Output format: (MovieID, (Rating, Number of Reviews))
(830, (1.0, 1.0))
(1308, (1.0, 1.0))
(1320, (1.0, 1.0))
(1330, (1.0, 1.0))
(1340, (1.0, 1.0))
(1348, (1.0, 1.0))
(1350, (1.0, 1.0))
(1364, (1.0, 1.0))
(1372, (1.0, 1.0))
(1430, (1.0, 1.0))
(1488, (1.0, 1.0))
(1494, (1.0, 1.0))
(1546, (1.0, 1.0))
(1562, (1.0, 1.0))
(1564, (1.0, 1.0))
(1566, (1.0, 1.0))
(1568, (1.0, 1.0))
(1574, (1.0, 1.0))
(1576, (1.0, 1.0))
(1580, (1.0, 1.0))
Total time to execute SparkRDD operations: 15 seconds.
18/11/05 18:08:14 INFO SparkContext: Invoking stop() from shutdown hook
18/11/05 18:08:14 INFO AbstractConnector: Stopped Spark@6acbf259{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
18/11/05 18:08:14 INFO SparkUI: Stopped Spark web UI at http://172.17.0.2:4041
18/11/05 18:08:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/11/05 18:08:15 INFO MemoryStore: MemoryStore cleared
18/11/05 18:08:15 INFO BlockManager: BlockManager stopped
18/11/05 18:08:15 INFO BlockManagerMaster: BlockManagerMaster stopped
18/11/05 18:08:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/11/05 18:08:15 INFO SparkContext: Successfully stopped SparkContext
18/11/05 18:08:15 INFO ShutdownHookManager: Shutdown hook called
18/11/05 18:08:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-ecbcbd72-949f-44ea-b122-0ea52cf94259/pyspark-02540a5a-b400-43b6-a2ed-f9f377da1126
18/11/05 18:08:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-ecbcbd72-949f-44ea-b122-0ea52cf94259